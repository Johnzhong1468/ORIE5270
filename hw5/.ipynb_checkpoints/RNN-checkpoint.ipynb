{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# RNN LSTM VS GRU\n",
    "#### Chen Zhong (John)\n",
    "cz379@cornell.edu\n",
    "\n",
    "### 1. Data Download and Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "\n",
    "# set the seed for reproduction\n",
    "SEED = 1234\n",
    "\n",
    "# set seed for torch process for either cpu or gpu devices\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  text field handles review and label handles sentiment\n",
    "* field determines how data is handled\n",
    "* text should be tokenized with 'spacy'\n",
    "* data should be processed as float tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField(tensor_type=torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* download IMDB data from datasets in torchtext\n",
    "* these data are torchtext.datasets objects then split data into train and test set\n",
    "* train set is then subset into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train, valid = train.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spare code to subset input\n",
    "\n",
    "# train_t, train_other = train.split(split_ratio=0.5, random_state=random.seed(SEED))\n",
    "# test_t, test_other = test.split(split_ratio=0.5, random_state=random.seed(SEED))\n",
    "# valid_t, valid_other = valid.split(split_ratio=0.5, random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* build vocabulary using the top 25000 most common words \n",
    "* apply one hot encoding to each word\n",
    "* initialize vector with pretrained embeddings where words in similar context appear in proximity in the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, max_size=25000, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sapred code for smaller dataset\n",
    "\n",
    "# TEXT.build_vocab(train_t, max_size=25000, vectors=\"glove.6B.100d\")\n",
    "# LABEL.build_vocab(train_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* create iterators\n",
    "* batch size determine how much data passes through the network each iteration\n",
    "* this step sorts the data into buckets of similar length and when iterator is called it returns a batch of examples from each bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train, valid, test), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.text), \n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spare code for smaller dataset\n",
    "\n",
    "# BATCH_SIZE = 16\n",
    "\n",
    "# train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "#     (train_t, valid_t, test_t), \n",
    "#     batch_size=BATCH_SIZE, \n",
    "#     sort_key=lambda x: len(x.text), \n",
    "#     repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Building the model\n",
    "* General process:\n",
    "    * takes one-hot vector as input\n",
    "    * embedding layer transforms input vector to a dense vector which will then enter the RNN algorithm\n",
    "* LSTM and GRU models, defines all specifications of model\n",
    "* First defnine 2 different model classes, detail explanation see in-line comments\n",
    "* LSTM package computes the following process:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "i_t &= \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
    "f_t &= \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
    "g_t &= \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
    "o_t &= \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
    "c_t &= f_t c_{(t-1)} + i_t g_t \\\\\n",
    "h_t &= o_t \\tanh(c_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "    * where h is the hidden state, c is the cell state, which incorporates long and short term information, x is the input. i, f, g, o are the input, forget, cell, and output gates, respectively. σ is the sigmoid function. (from official document of torch.nn package)\n",
    "* GRU package computes the following process:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "r_t &= \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n",
    "z_t &= \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\n",
    "n_t &= \\tanh(W_{in} x_t + b_{in} + r_t (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\n",
    "h_t &= (1 - z_t) n_t + z_t h_{(t-1)} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "    * where h is the hidden state, x is the input. r, z, n are the reset, update, and new gates, respectively. σ is the sigmoid function. (from official document of torch.nn package)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        vocab_size: input dimension, dimension of one-hot vector, which is length of TEXT.vocab\n",
    "        embedding_dim: dimension of the dense word vector post embedding\n",
    "        hidden_dim: size of hidden states, hidden states are layers for previous inputs to pass through to get updated values\n",
    "        output_dim: dimension of output class, we only need a real value 0-1 \n",
    "        n_layers: number of layers in the neural network, more than 1 is called deep neural network, \n",
    "            output of hidden state in first layer is the input to the hidden state in the next layer\n",
    "        bidirectional: adds an extra layer that processes values from last to first, where originally only from first to last\n",
    "        dropout: a regularization method to avoid overfitting, randomly dropout a node from the forward process, getting less\n",
    "            parameters and hence avoid over parameterization\n",
    "        \"\"\"\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # LSTM package that takes in specifications\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        # defines forward process, bidirectional requires the square of hidden dimension\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        defines the forwarding process between each node\n",
    "        note that LSTM output specifies cell state\n",
    "        \"\"\"\n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        # regularization in the embedding process\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        # output of the LSTM RNN process in each node, including the output, new hidden state, and cell state\n",
    "        output, (hidden, cell) = self.rnn(embedded)        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid. dim]\n",
    "        #cell = [num layers * num directions, batch size, hid. dim]\n",
    "        \n",
    "        # regularize the hidden state to avoid overfitting\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "                \n",
    "        #hidden [batch size, hid. dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden.squeeze(0))\n",
    "\n",
    "\n",
    "class RNN_GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        same as LSTM specification, instead we use GRU package\n",
    "        \"\"\"\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        difference between LSTM and GRU here is that GRU output does not have cell state,\n",
    "        as we can see from mathematical definition above\n",
    "        \"\"\"\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid. dim]\n",
    "        #cell = [num layers * num directions, batch size, hid. dim]\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "                \n",
    "        #hidden [batch size, hid. dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation\n",
    "1. setup parameters\n",
    "2. pass parameters to class and build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model_lstm = RNN_LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "model_gru = RNN_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check size of pretrained embeddings\n",
    "\"\"\"\n",
    "\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assign pretrained embeddings to embedding layer for 2 separate models\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.1123,  0.3113,  0.3317,  ..., -0.4576,  0.6191,  0.5304],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.1123,  0.3113,  0.3317,  ..., -0.4576,  0.6191,  0.5304],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gru.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\"\"\"\n",
    "use Adam optimization algorithm\n",
    "a stochastic optimization algorithm\n",
    "\"\"\"\n",
    "optimizer_lstm = optim.Adam(model_lstm.parameters())\n",
    "optimizer_gru = optim.Adam(model_gru.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "specify loss function: BCE with logits loss\n",
    "\"\"\"\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\"\"\"\n",
    "use GPU if availbale, otherwise use CPU\n",
    "\"\"\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_lstm = model_lstm.to(device)\n",
    "model_gru = model_gru.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as F\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(F.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, iterator, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    main train function to train with each batch in iterator, iterates through all examples\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \"\"\"\n",
    "        optimization step\n",
    "        \"\"\"\n",
    "        \n",
    "        # first zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # feed batch of sentences to model\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        # calculate gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    main function for evaluation\n",
    "    similar to train\n",
    "    do not need to zero gradients\n",
    "    do not update parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0429z\\Anaconda3\\lib\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN-LSTM training data\n",
      "Epoch: 01, Train Loss: 0.432, Train Acc: 80.26%, Val. Loss: 0.304, Val. Acc: 88.12%\n",
      "RNN-LSTM training data\n",
      "Epoch: 02, Train Loss: 0.246, Train Acc: 90.56%, Val. Loss: 0.273, Val. Acc: 89.18%\n",
      "RNN-LSTM training data\n",
      "Epoch: 03, Train Loss: 0.167, Train Acc: 93.89%, Val. Loss: 0.279, Val. Acc: 88.32%\n",
      "RNN-LSTM training data\n",
      "Epoch: 04, Train Loss: 0.117, Train Acc: 95.86%, Val. Loss: 0.312, Val. Acc: 89.17%\n",
      "RNN-LSTM training data\n",
      "Epoch: 05, Train Loss: 0.085, Train Acc: 97.20%, Val. Loss: 0.369, Val. Acc: 89.13%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\"\"\"\n",
    "train model for 5 epochs and output training statistics and validation statstics\n",
    "\"\"\"\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss_lstm, train_acc_lstm = train_model(model_lstm, train_iterator, optimizer_lstm, criterion)\n",
    "    valid_loss_lstm, valid_acc_lstm = evaluate(model_lstm, valid_iterator, criterion)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"RNN-LSTM training data\")\n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss_lstm:.3f}, Train Acc: {train_acc_lstm*100:.2f}%, Val. Loss: {valid_loss_lstm:.3f}, Val. Acc: {valid_acc_lstm*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0429z\\Anaconda3\\lib\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN-LSTM test result\n",
      "Test Loss: 0.478, Test Acc: 86.23%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "test final model\n",
    "\"\"\"\n",
    "test_loss_lstm, test_acc_lstm = evaluate(model_lstm, test_iterator, criterion)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"RNN-LSTM test result\")\n",
    "print(f'Test Loss: {test_loss_lstm:.3f}, Test Acc: {test_acc_lstm*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN-GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0429z\\Anaconda3\\lib\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN-GRU training data\n",
      "Epoch: 01, Train Loss: 0.601, Train Acc: 64.28%, Val. Loss: 0.301, Val. Acc: 87.69%\n",
      "RNN-GRU training data\n",
      "Epoch: 02, Train Loss: 0.268, Train Acc: 88.99%, Val. Loss: 0.232, Val. Acc: 90.45%\n",
      "RNN-GRU training data\n",
      "Epoch: 03, Train Loss: 0.176, Train Acc: 93.35%, Val. Loss: 0.246, Val. Acc: 90.06%\n",
      "RNN-GRU training data\n",
      "Epoch: 04, Train Loss: 0.125, Train Acc: 95.60%, Val. Loss: 0.273, Val. Acc: 90.03%\n",
      "RNN-GRU training data\n",
      "Epoch: 05, Train Loss: 0.090, Train Acc: 96.79%, Val. Loss: 0.290, Val. Acc: 89.90%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss_gru, train_acc_gru = train_model(model_gru, train_iterator, optimizer_gru, criterion)\n",
    "    valid_loss_gru, valid_acc_gru = evaluate(model_gru, valid_iterator, criterion)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"RNN-GRU training data\")\n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss_gru:.3f}, Train Acc: {train_acc_gru*100:.2f}%, Val. Loss: {valid_loss_gru:.3f}, Val. Acc: {valid_acc_gru*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0429z\\Anaconda3\\lib\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN-GRU test result\n",
      "Test Loss: 0.375, Test Acc: 87.33%\n"
     ]
    }
   ],
   "source": [
    "test_loss_gru, test_acc_gru = evaluate(model_gru, test_iterator, criterion)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"RNN-GRU test result\")\n",
    "print(f'Test Loss: {test_loss_gru:.3f}, Test Acc: {test_acc_gru*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Using the IMDB dataset, GRU yields slightly more accurate (1% more) result than LSTM, though both training, validationa and test results are with 1% of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Implementing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment_lstm(sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction_lstm = F.sigmoid(model_lstm(tensor))\n",
    "    return prediction_lstm.item()\n",
    "\n",
    "def predict_sentiment_gru(sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction_gru = F.sigmoid(model_gru(tensor))\n",
    "    return prediction_gru.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9209697842597961"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment_lstm(\"The result is hugely enjoyable, and hooray for Hollywood for making it happen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00314916018396616"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment_lstm(\"A disordered and unfocused ghost story that bears all the very worst habits of the genre.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6537795662879944"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment_gru(\"The result is hugely enjoyable, and hooray for Hollywood for making it happen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01172313466668129"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment_gru(\"A disordered and unfocused ghost story that bears all the very worst habits of the genre.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
